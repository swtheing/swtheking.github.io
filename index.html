<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Policy Gradient in Pong Game by swtheing</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Policy Gradient in Pong Game</h1>
      <h2 class="project-tagline">计算机界2009的技术博客</h2>
      <a href="https://github.com/swtheing/swtheking.github.io" class="btn">View on GitHub</a>
      <a href="https://github.com/swtheing/swtheking.github.io/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/swtheing/swtheking.github.io/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="什么是deep-reinforcement-learning-" class="anchor" href="#%E4%BB%80%E4%B9%88%E6%98%AFdeep-reinforcement-learning-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>什么是Deep Reinforcement Learning ？</h1>

<p>Deep Reinforce Learning可以从下面几个资料中获得，</p>

<ul>
<li>
<a href="https://webdocs.cs.ualberta.ca/%7Esutton/book/the-book.html">Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto</a>，这本书可以说是整个Reinforcement Learning最权威的书籍，尤其第二版加入了很多关于Deep的东西。</li>
<li>
<a href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf">Deep Reinforcement Learning. David Silver, Google DeepMind</a>，这是DeepMind一个ppt主要讲的是Deep Q-Network。</li>
<li>
<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL的一个关于Deep Reinforcement Learning的课程</a>。</li>
<li>
<a href="http://karpathy.github.io/2016/05/31/rl/">Policy Gradient Tutoiral</a>，本文的代码以及解释来源于此。</li>
</ul>

<h1>
<a id="policy-gradient" class="anchor" href="#policy-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Policy Gradient</h1>

<h2>
<a id="what-is-policy-gradient" class="anchor" href="#what-is-policy-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>what is policy gradient</h2>

<p>在笔者看来，Policy Gradient是对于一个Policy的求导，也就是对于一个映射函数（state -&gt; action）的求导。</p>

<h2>
<a id="why-policy-gradient" class="anchor" href="#why-policy-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>why policy gradient</h2>

<p>Policy Gradient的出现是因为Deep Learning的出现，否则，怎么会有人想到为一个Policy函数求Gradient呢。相比于Deep Q-learning，Policy Gradient更容易实现，也更容易做BP，如果想了解细节，请看<a href="http://karpathy.github.io/2016/05/31/rl/">Policy Gradient Tutoiral</a>。</p>

<h2>
<a id="loss-of-policy-gradient" class="anchor" href="#loss-of-policy-gradient" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Loss of Policy Gradient</h2>

<p>其实Policy Gradient的Loss是Log Loss，也就是Cross Entropy，但是有一个trick的地方是，Cross Entropy计算出的gradient的只是一个方向，它的值需要再乘以discount reward，具体数学可以看<a href="http://karpathy.github.io/2016/05/31/rl/">Policy Gradient Tutoiral</a>，代码部分解释可以看下面。</p>

<h1>
<a id="代码解读" class="anchor" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5">代码解读</a>
</h1>

<ul>
<li>observation 代码，<code>prepro(I)</code>函数中，做了一些剔除背景的操作。</li>
</ul>

<div class="highlight highlight-source-js"><pre>def <span class="pl-en">prepro</span>(<span class="pl-c1">I</span>)<span class="pl-k">:</span>
    <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span> prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector <span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
    <span class="pl-c1">I</span> <span class="pl-k">=</span> <span class="pl-c1">I</span>[<span class="pl-c1">35</span><span class="pl-k">:</span><span class="pl-c1">195</span>] # crop
    <span class="pl-c1">I</span> <span class="pl-k">=</span> <span class="pl-c1">I</span>[<span class="pl-k">::</span><span class="pl-c1">2</span>,<span class="pl-k">::</span><span class="pl-c1">2</span>,<span class="pl-c1">0</span>] # downsample by factor <span class="pl-k">of</span> <span class="pl-c1">2</span>
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">==</span> <span class="pl-c1">144</span>] <span class="pl-k">=</span> <span class="pl-c1">0</span>  # erase <span class="pl-en">background</span> (background type <span class="pl-c1">1</span>)
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">==</span> <span class="pl-c1">109</span>] <span class="pl-k">=</span> <span class="pl-c1">0</span>  # erase <span class="pl-en">background</span> (background type <span class="pl-c1">2</span>)
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">!=</span> <span class="pl-c1">0</span>] <span class="pl-k">=</span> <span class="pl-c1">1</span>    # everything <span class="pl-k">else</span> (paddles, ball) just set to <span class="pl-c1">1</span>
    <span class="pl-k">return</span> <span class="pl-c1">I</span>.<span class="pl-en">astype</span>(<span class="pl-smi">np</span>.<span class="pl-smi">float</span>).<span class="pl-en">ravel</span>()</pre></div>

<ul>
<li>计算discount_reward代码，其中对于<code>[0,0,0,1]</code>这样的得分数组，如果设<code>gamma = 0.9</code>，那么得到打折后的得分是<code>[0.729,0.81,0.9,1]</code>这样的打折分数：</li>
</ul>

<div class="highlight highlight-source-js"><pre>def <span class="pl-en">discount_rewards</span>(r)<span class="pl-k">:</span>
  <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span> take 1D float array of rewards and compute discounted reward <span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
  discounted_r <span class="pl-k">=</span> <span class="pl-smi">np</span>.<span class="pl-en">zeros_like</span>(r)
  running_add <span class="pl-k">=</span> <span class="pl-c1">0</span>
  <span class="pl-k">for</span> t <span class="pl-k">in</span> <span class="pl-en">reversed</span>(<span class="pl-en">xrange</span>(<span class="pl-c1">0</span>, <span class="pl-smi">r</span>.<span class="pl-c1">size</span>))<span class="pl-k">:</span>
    <span class="pl-k">if</span> r[t] <span class="pl-k">!=</span> <span class="pl-c1">0</span><span class="pl-k">:</span> running_add <span class="pl-k">=</span> <span class="pl-c1">0</span> # reset the sum, since <span class="pl-v">this</span> was a game <span class="pl-en">boundary</span> (pong specific<span class="pl-k">!</span>)
    running_add <span class="pl-k">=</span> running_add <span class="pl-k">*</span> gamma <span class="pl-k">+</span> r[t]
    discounted_r[t] <span class="pl-k">=</span> running_add
  <span class="pl-k">return</span> discounted_r</pre></div>

<ul>
<li>
<code>Policy Forward</code> 和 <code>Policy Backward</code>算法，就是普通的foward 和 BP算法。例如backward中的<code>dW2 = np.dot(eph.T, epdlogp).ravel()</code>就是使用了链式法则去求Gradient。</li>
<li>下面一段是Open AI Gym的一些配置，只要知道<code>render</code>是gym中是否显示图像的开关，如果在服务器上训练，请关掉。</li>
<li>下面代码是整个文章的核心，首先声明一点，在原文章代码中它只考虑action= 2 or 3的两种情况（上或者下），也就是没有考虑1，也就是停止的情况。因此在计算Gradient的时候，你会发现它使用的Gradient方向是<code>y-aprob</code>，为什么如此，因为我们可以看一下整个aprob计算，它是通过sigmoid函数计算出概率值aprob。因此，我们就是计算 ylog(aprob) + (1-y)log(1-aprob)的倒数值，又 aprob = σ(x)，所以结果是<code>y-aprob</code>，<a href="http://cs231n.github.io/neural-networks-2/#losses">具体可以看一下logloss的gradient推倒方法</a>
</li>
</ul>

<div class="highlight highlight-source-js"><pre><span class="pl-k">while</span> True<span class="pl-k">:</span>
  <span class="pl-k">if</span> render<span class="pl-k">:</span> <span class="pl-smi">env</span>.<span class="pl-en">render</span>()

  # preprocess the observation, set input to network to be difference image
  cur_x <span class="pl-k">=</span> <span class="pl-en">prepro</span>(observation)
  x <span class="pl-k">=</span> cur_x <span class="pl-k">-</span> prev_x <span class="pl-k">if</span> prev_x is not None <span class="pl-k">else</span> <span class="pl-smi">np</span>.<span class="pl-en">zeros</span>(<span class="pl-c1">D</span>)
  prev_x <span class="pl-k">=</span> cur_x

  # forward the policy network and sample an action from the returned probability
  aprob, h <span class="pl-k">=</span> <span class="pl-en">policy_forward</span>(x)
  action <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">if</span> <span class="pl-smi">np</span>.<span class="pl-smi">random</span>.<span class="pl-en">uniform</span>() <span class="pl-k">&lt;</span> aprob <span class="pl-k">else</span> <span class="pl-c1">3</span> # roll the dice<span class="pl-k">!</span>

  # record various <span class="pl-en">intermediates</span> (needed later <span class="pl-k">for</span> backprop)
  <span class="pl-smi">xs</span>.<span class="pl-c1">append</span>(x) # observation
  <span class="pl-smi">hs</span>.<span class="pl-c1">append</span>(h) # hidden state
  y <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-k">if</span> action <span class="pl-k">==</span> <span class="pl-c1">2</span> <span class="pl-k">else</span> <span class="pl-c1">0</span> # a <span class="pl-s"><span class="pl-pds">"</span>fake label<span class="pl-pds">"</span></span>
  <span class="pl-smi">dlogps</span>.<span class="pl-c1">append</span>(y <span class="pl-k">-</span> aprob) # grad that encourages the action that was taken to be <span class="pl-en">taken</span> (see http<span class="pl-k">:</span><span class="pl-c">//cs231n.github.io/neural-networks-2/#losses if confused)</span>

  # step the environment and get <span class="pl-k">new</span> <span class="pl-en">measurements</span>
  observation, reward, done, info <span class="pl-k">=</span> <span class="pl-smi">env</span>.<span class="pl-en">step</span>(action)
  reward_sum <span class="pl-k">+=</span> reward

  <span class="pl-smi">drs</span>.<span class="pl-c1">append</span>(reward) </pre></div>

<ul>
<li>最后其实就是如何存储Gradient以及如何计算batch Gradient的方法，大家可以依据很多深度学习资料比对一下，至于running_reward的公式的原理，大家可以看看Deep Reinforcement Learning的书籍。</li>
</ul>

<div class="highlight highlight-source-js"><pre>    <span class="pl-k">if</span> episode_number <span class="pl-k">%</span> batch_size <span class="pl-k">==</span> <span class="pl-c1">0</span><span class="pl-k">:</span>
      <span class="pl-k">for</span> k,v <span class="pl-k">in</span> <span class="pl-smi">model</span>.<span class="pl-en">iteritems</span>()<span class="pl-k">:</span>
        g <span class="pl-k">=</span> grad_buffer[k] # gradient
        rmsprop_cache[k] <span class="pl-k">=</span> decay_rate <span class="pl-k">*</span> rmsprop_cache[k] <span class="pl-k">+</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> decay_rate) <span class="pl-k">*</span> g<span class="pl-k">**</span><span class="pl-c1">2</span>
        model[k] <span class="pl-k">+=</span> learning_rate <span class="pl-k">*</span> g <span class="pl-k">/</span> (<span class="pl-smi">np</span>.<span class="pl-en">sqrt</span>(rmsprop_cache[k]) <span class="pl-k">+</span> <span class="pl-c1">1e-5</span>)
        grad_buffer[k] <span class="pl-k">=</span> <span class="pl-smi">np</span>.<span class="pl-en">zeros_like</span>(v) # reset batch gradient buffer

    # boring book<span class="pl-k">-</span>keeping
    running_reward <span class="pl-k">=</span> reward_sum <span class="pl-k">if</span> running_reward is None <span class="pl-k">else</span> running_reward <span class="pl-k">*</span> <span class="pl-c1">0.99</span> <span class="pl-k">+</span> reward_sum <span class="pl-k">*</span> <span class="pl-c1">0.01</span></pre></div>

<h1>
<a id="policy-gradient-in-pong-game-with-tensorflow" class="anchor" href="#policy-gradient-in-pong-game-with-tensorflow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Policy Gradient in Pong Game with Tensorflow</h1>

<p>这段代码是我根据网上一个人的代码改写的，他写的代码虽然说是Policy Gradient，但实际只用了L2的loss，其实是错误的。</p>

<div class="highlight highlight-source-js"><pre><span class="pl-k">import</span> <span class="pl-smi">numpy</span> <span class="pl-k">as</span> <span class="pl-smi">np</span>
<span class="pl-k">import</span> gym
<span class="pl-k">import</span> <span class="pl-smi">tensorflow</span> <span class="pl-k">as</span> <span class="pl-smi">tf</span>

# hyperparameters
n_obs <span class="pl-k">=</span> <span class="pl-c1">80</span> <span class="pl-k">*</span> <span class="pl-c1">80</span>           # dimensionality <span class="pl-k">of</span> observations
h <span class="pl-k">=</span> <span class="pl-c1">200</span>                   # number <span class="pl-k">of</span> hidden layer neurons
n_actions <span class="pl-k">=</span> <span class="pl-c1">3</span>             # number <span class="pl-k">of</span> available actions
learning_rate <span class="pl-k">=</span> <span class="pl-c1">1e-3</span>
gamma <span class="pl-k">=</span> <span class="pl-c1">.99</span>               # discount factor <span class="pl-k">for</span> reward
decay <span class="pl-k">=</span> <span class="pl-c1">0.99</span>              # decay rate <span class="pl-k">for</span> RMSProp gradients
save_path<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>models/pong.ckpt<span class="pl-pds">'</span></span>

# gamespace 
env <span class="pl-k">=</span> <span class="pl-smi">gym</span>.<span class="pl-en">make</span>(<span class="pl-s"><span class="pl-pds">"</span>Pong-v0<span class="pl-pds">"</span></span>) # environment info
observation <span class="pl-k">=</span> <span class="pl-smi">env</span>.<span class="pl-c1">reset</span>()
prev_x <span class="pl-k">=</span> None
xs,rs,ys <span class="pl-k">=</span> [],[],[]
running_reward <span class="pl-k">=</span> None
reward_sum <span class="pl-k">=</span> <span class="pl-c1">0</span>
episode_number <span class="pl-k">=</span> <span class="pl-c1">0</span>

# initialize model
tf_model <span class="pl-k">=</span> {}
<span class="pl-k">with</span> <span class="pl-smi">tf</span>.<span class="pl-en">variable_scope</span>(<span class="pl-s"><span class="pl-pds">'</span>layer_one<span class="pl-pds">'</span></span>,reuse<span class="pl-k">=</span>False)<span class="pl-k">:</span>
    xavier_l1 <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">truncated_normal_initializer</span>(mean<span class="pl-k">=</span><span class="pl-c1">0</span>, stddev<span class="pl-k">=</span><span class="pl-c1">1.</span><span class="pl-k">/</span><span class="pl-smi">np</span>.<span class="pl-en">sqrt</span>(n_obs), dtype<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-smi">float32</span>)
    tf_model[<span class="pl-s"><span class="pl-pds">'</span>W1<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">get_variable</span>(<span class="pl-s"><span class="pl-pds">"</span>W1<span class="pl-pds">"</span></span>, [n_obs, h], initializer<span class="pl-k">=</span>xavier_l1)
<span class="pl-k">with</span> <span class="pl-smi">tf</span>.<span class="pl-en">variable_scope</span>(<span class="pl-s"><span class="pl-pds">'</span>layer_two<span class="pl-pds">'</span></span>,reuse<span class="pl-k">=</span>False)<span class="pl-k">:</span>
    xavier_l2 <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">truncated_normal_initializer</span>(mean<span class="pl-k">=</span><span class="pl-c1">0</span>, stddev<span class="pl-k">=</span><span class="pl-c1">1.</span><span class="pl-k">/</span><span class="pl-smi">np</span>.<span class="pl-en">sqrt</span>(h), dtype<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-smi">float32</span>)
    tf_model[<span class="pl-s"><span class="pl-pds">'</span>W2<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">get_variable</span>(<span class="pl-s"><span class="pl-pds">"</span>W2<span class="pl-pds">"</span></span>, [h,n_actions], initializer<span class="pl-k">=</span>xavier_l2)

# tf operations
def <span class="pl-en">discount_rewards</span>(r)<span class="pl-k">:</span>
  <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span> take 1D float array of rewards and compute discounted reward <span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
  discounted_r <span class="pl-k">=</span> <span class="pl-smi">np</span>.<span class="pl-en">zeros_like</span>(r)
  running_add <span class="pl-k">=</span> <span class="pl-c1">0</span>
  <span class="pl-k">for</span> t <span class="pl-k">in</span> <span class="pl-en">reversed</span>(<span class="pl-en">xrange</span>(<span class="pl-c1">0</span>, <span class="pl-smi">r</span>.<span class="pl-c1">size</span>))<span class="pl-k">:</span>
    <span class="pl-k">if</span> r[t] <span class="pl-k">!=</span> <span class="pl-c1">0</span><span class="pl-k">:</span> running_add <span class="pl-k">=</span> <span class="pl-c1">0</span> # reset the sum, since <span class="pl-v">this</span> was a game <span class="pl-en">boundary</span> (pong specific<span class="pl-k">!</span>)
    running_add <span class="pl-k">=</span> running_add <span class="pl-k">*</span> gamma <span class="pl-k">+</span> r[t]
    discounted_r[t] <span class="pl-k">=</span> running_add
  <span class="pl-k">return</span> discounted_r

def <span class="pl-en">tf_policy_forward</span>(x)<span class="pl-k">:</span> #x <span class="pl-k">~</span> [<span class="pl-c1">1</span>,<span class="pl-c1">D</span>]
    h <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">matmul</span>(x, tf_model[<span class="pl-s"><span class="pl-pds">'</span>W1<span class="pl-pds">'</span></span>])
    h <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">nn</span>.<span class="pl-en">relu</span>(h)
    logp <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">matmul</span>(h, tf_model[<span class="pl-s"><span class="pl-pds">'</span>W2<span class="pl-pds">'</span></span>])
    p <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">nn</span>.<span class="pl-en">softmax</span>(logp)
    <span class="pl-k">return</span> p

# downsampling
def <span class="pl-en">prepro</span>(<span class="pl-c1">I</span>)<span class="pl-k">:</span>
    <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span> prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector <span class="pl-pds">"</span><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
    <span class="pl-c1">I</span> <span class="pl-k">=</span> <span class="pl-c1">I</span>[<span class="pl-c1">35</span><span class="pl-k">:</span><span class="pl-c1">195</span>] # crop
    <span class="pl-c1">I</span> <span class="pl-k">=</span> <span class="pl-c1">I</span>[<span class="pl-k">::</span><span class="pl-c1">2</span>,<span class="pl-k">::</span><span class="pl-c1">2</span>,<span class="pl-c1">0</span>] # downsample by factor <span class="pl-k">of</span> <span class="pl-c1">2</span>
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">==</span> <span class="pl-c1">144</span>] <span class="pl-k">=</span> <span class="pl-c1">0</span>  # erase <span class="pl-en">background</span> (background type <span class="pl-c1">1</span>)
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">==</span> <span class="pl-c1">109</span>] <span class="pl-k">=</span> <span class="pl-c1">0</span>  # erase <span class="pl-en">background</span> (background type <span class="pl-c1">2</span>)
    <span class="pl-c1">I</span>[<span class="pl-c1">I</span> <span class="pl-k">!=</span> <span class="pl-c1">0</span>] <span class="pl-k">=</span> <span class="pl-c1">1</span>    # everything <span class="pl-k">else</span> (paddles, ball) just set to <span class="pl-c1">1</span>
    <span class="pl-k">return</span> <span class="pl-c1">I</span>.<span class="pl-en">astype</span>(<span class="pl-smi">np</span>.<span class="pl-smi">float</span>).<span class="pl-en">ravel</span>()

# tf placeholders
tf_x <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">placeholder</span>(dtype<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-smi">float32</span>, shape<span class="pl-k">=</span>[None, n_obs],name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>tf_x<span class="pl-pds">"</span></span>)
tf_y <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">placeholder</span>(dtype<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-smi">int32</span>, shape<span class="pl-k">=</span>[None],name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>tf_y<span class="pl-pds">"</span></span>)
tf_epr <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">placeholder</span>(dtype<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-smi">float32</span>, shape<span class="pl-k">=</span>[None], name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>tf_epr<span class="pl-pds">"</span></span>)

# tf optimizer op
tf_aprob <span class="pl-k">=</span> <span class="pl-en">tf_policy_forward</span>(tf_x)
cross_entropy <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">nn</span>.<span class="pl-en">sparse_softmax_cross_entropy_with_logits</span>(tf_aprob, tf_y)
loss <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">reduce_sum</span>(<span class="pl-smi">tf</span>.<span class="pl-en">mul</span>(cross_entropy,tf_epr))
optimizer <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">train</span>.<span class="pl-en">RMSPropOptimizer</span>(learning_rate, decay<span class="pl-k">=</span>decay)
tf_grads <span class="pl-k">=</span> <span class="pl-smi">optimizer</span>.<span class="pl-en">compute_gradients</span>(loss, var_list<span class="pl-k">=</span><span class="pl-smi">tf</span>.<span class="pl-en">trainable_variables</span>())
train_op <span class="pl-k">=</span> <span class="pl-smi">optimizer</span>.<span class="pl-en">apply_gradients</span>(tf_grads)
#<span class="pl-en">print</span>(tf_grads)

# tf graph initialization
sess <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-en">InteractiveSession</span>()
<span class="pl-smi">tf</span>.<span class="pl-en">initialize_all_variables</span>().<span class="pl-en">run</span>()

# <span class="pl-k">try</span> load saved model
saver <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">train</span>.<span class="pl-en">Saver</span>(<span class="pl-smi">tf</span>.<span class="pl-en">all_variables</span>())
load_was_success <span class="pl-k">=</span> True
try<span class="pl-k">:</span>
    save_dir <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>/<span class="pl-pds">'</span></span>.<span class="pl-c1">join</span>(<span class="pl-smi">save_path</span>.<span class="pl-c1">split</span>(<span class="pl-s"><span class="pl-pds">'</span>/<span class="pl-pds">'</span></span>)[<span class="pl-k">:</span><span class="pl-k">-</span><span class="pl-c1">1</span>])
    ckpt <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">train</span>.<span class="pl-en">get_checkpoint_state</span>(save_dir)
    load_path <span class="pl-k">=</span> <span class="pl-smi">ckpt</span>.<span class="pl-smi">model_checkpoint_path</span>
    <span class="pl-smi">saver</span>.<span class="pl-en">restore</span>(sess, load_path)
except<span class="pl-k">:</span>
    print <span class="pl-s"><span class="pl-pds">"</span>no saved model to load. starting new session<span class="pl-pds">"</span></span>
    load_was_success <span class="pl-k">=</span> False
else<span class="pl-k">:</span>
    print <span class="pl-s"><span class="pl-pds">"</span>loaded model: {}<span class="pl-pds">"</span></span>.<span class="pl-en">format</span>(load_path)
    saver <span class="pl-k">=</span> <span class="pl-smi">tf</span>.<span class="pl-smi">train</span>.<span class="pl-en">Saver</span>(<span class="pl-smi">tf</span>.<span class="pl-en">all_variables</span>())
    episode_number <span class="pl-k">=</span> <span class="pl-en">int</span>(<span class="pl-smi">load_path</span>.<span class="pl-c1">split</span>(<span class="pl-s"><span class="pl-pds">'</span>-<span class="pl-pds">'</span></span>)[<span class="pl-k">-</span><span class="pl-c1">1</span>])


# training loop
<span class="pl-k">while</span> True<span class="pl-k">:</span>
#     <span class="pl-k">if</span> True<span class="pl-k">:</span> <span class="pl-smi">env</span>.<span class="pl-en">render</span>()

    # preprocess the observation, set input to network to be difference image
    cur_x <span class="pl-k">=</span> <span class="pl-en">prepro</span>(observation)
    x <span class="pl-k">=</span> cur_x <span class="pl-k">-</span> prev_x <span class="pl-k">if</span> prev_x is not None <span class="pl-k">else</span> <span class="pl-smi">np</span>.<span class="pl-en">zeros</span>(n_obs)
    prev_x <span class="pl-k">=</span> cur_x

    # stochastically sample a policy from the network
    feed <span class="pl-k">=</span> {tf_x<span class="pl-k">:</span> <span class="pl-smi">np</span>.<span class="pl-en">reshape</span>(x, (<span class="pl-c1">1</span>,<span class="pl-k">-</span><span class="pl-c1">1</span>))}
    aprob <span class="pl-k">=</span> <span class="pl-smi">sess</span>.<span class="pl-en">run</span>(tf_aprob,feed)
    aprob <span class="pl-k">=</span> aprob[<span class="pl-c1">0</span>,<span class="pl-k">:</span>]
    action <span class="pl-k">=</span> <span class="pl-smi">np</span>.<span class="pl-smi">random</span>.<span class="pl-en">choice</span>(n_actions, p<span class="pl-k">=</span>aprob)

    # step the environment and get <span class="pl-k">new</span> <span class="pl-en">measurements</span>
    observation, reward, done, info <span class="pl-k">=</span> <span class="pl-smi">env</span>.<span class="pl-en">step</span>(action<span class="pl-k">+</span><span class="pl-c1">1</span>)
    reward_sum <span class="pl-k">+=</span> reward

    # record game history
    <span class="pl-smi">xs</span>.<span class="pl-c1">append</span>(x) ; <span class="pl-smi">ys</span>.<span class="pl-c1">append</span>(action) ; <span class="pl-smi">rs</span>.<span class="pl-c1">append</span>(reward)

    <span class="pl-k">if</span> done<span class="pl-k">:</span>
        # update running reward
        running_reward <span class="pl-k">=</span> reward_sum <span class="pl-k">if</span> running_reward is None <span class="pl-k">else</span> running_reward <span class="pl-k">*</span> <span class="pl-c1">0.99</span> <span class="pl-k">+</span> reward_sum <span class="pl-k">*</span> <span class="pl-c1">0.01</span>
        dis <span class="pl-k">=</span> <span class="pl-en">discount_rewards</span>(<span class="pl-smi">np</span>.<span class="pl-en">asarray</span>(rs))
        dis <span class="pl-k">-=</span> <span class="pl-smi">np</span>.<span class="pl-en">mean</span>(dis)
        dis <span class="pl-k">/=</span> <span class="pl-smi">np</span>.<span class="pl-en">std</span>(dis)

        # parameter update
        feed <span class="pl-k">=</span> {tf_x<span class="pl-k">:</span> <span class="pl-smi">np</span>.<span class="pl-en">vstack</span>(xs), tf_epr<span class="pl-k">:</span> dis, tf_y<span class="pl-k">:</span> <span class="pl-smi">np</span>.<span class="pl-en">asarray</span>(ys)}
        _,loss_val <span class="pl-k">=</span> <span class="pl-smi">sess</span>.<span class="pl-en">run</span>([train_op,loss],feed)
        <span class="pl-en">print</span>(loss_val)


        # print progress <span class="pl-en">console</span>
        <span class="pl-k">if</span> episode_number <span class="pl-k">%</span> <span class="pl-c1">10</span> <span class="pl-k">==</span> <span class="pl-c1">0</span><span class="pl-k">:</span>
            print <span class="pl-s"><span class="pl-pds">'</span>ep {}: reward: {}, mean reward: {:3f}<span class="pl-pds">'</span></span>.<span class="pl-en">format</span>(episode_number, reward_sum, running_reward)
        else<span class="pl-k">:</span>
            print <span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\t</span>ep {}: reward: {}<span class="pl-pds">'</span></span>.<span class="pl-en">format</span>(episode_number, reward_sum)

        # bookkeeping
        xs,rs,ys <span class="pl-k">=</span> [],[],[] # reset game history
        episode_number <span class="pl-k">+=</span> <span class="pl-c1">1</span> # The Next Episode
        observation <span class="pl-k">=</span> <span class="pl-smi">env</span>.<span class="pl-c1">reset</span>() # reset env
        reward_sum <span class="pl-k">=</span> <span class="pl-c1">0</span>
        <span class="pl-k">if</span> episode_number <span class="pl-k">%</span> <span class="pl-c1">50</span> <span class="pl-k">==</span> <span class="pl-c1">0</span><span class="pl-k">:</span>
            <span class="pl-smi">saver</span>.<span class="pl-en">save</span>(sess, save_path, global_step<span class="pl-k">=</span>episode_number)
            print <span class="pl-s"><span class="pl-pds">"</span>SAVED MODEL #{}<span class="pl-pds">"</span></span>.<span class="pl-en">format</span>(episode_number)</pre></div>

<p>（未完待续）</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/swtheing/swtheking.github.io">Policy Gradient in Pong Game</a> is maintained by <a href="https://github.com/swtheing">swtheing</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
